load("~/Work/DSCoursera/datasciencecoursera/barbell/project.Rmd")
library("caret")
set.seed(666)
# Read Data
pmlData <- read.csv("pml-training.csv")
pmlFinalTest <- read.csv("pml-testing.csv")
View(pmlData)
View(pmlFinalTest)
names(pmlData)
names(pmlFinalTest)
names(pmlFinalTest) != names(pmlData)
names(pmlFinalTest) in names(pmlData)
names(pmlFinalTest)
names(pmlFinalTest)[0]
names(pmlFinalTest)[1]
intersect(pmlData,pmlFinalTest)
intersect(names(pmlData),names(pmlFinalTest))
?intersect
common = intersect(names(pmlData),names(pmlFinalTest))
pmlData[pmlData$new_windows == 'new', ]
pmlData[pmlData$new_windows == 'new']
pmlData[pmlData$new_window == 'new', ]
pmlData[pmlData$new_window = 'new', ]
pmlData[pmlData$new_window == 'new' ]
pmlData$new_window
pmlData[pmlData$new_window == 'yes' ]
pmlData[pmlData$new_window == 'yes', ]
data = pmlData[pmlData$new_window == 'yes', ]
View(data)
complete.cases(data)
!complete.cases(data)
sum(!complete.cases(data))
str(data)
inTrain <- createDataPartition(y = data$classe, p = 0.8, list = F)
trainning <- pmlData[inTrain,]
testing <- pmlData[-inTrain,]
system.time({
modelFit <- train(classe ~ .,
data = trainning,
#trControl=train_control, ntree=100,
verbose=FALSE,
method=c("rf"))
})
warnings()
system.time({
modelFit <- train(classe ~ .,
data = trainning,
#trControl=train_control, ntree=100,
verbose=FALSE,
method=c("glm"))
})
warnings()
str(data)
drop = c('X', 'user_name', 'raw_timestamp_part_1', 'raw_timestamp_part_2','cvtd_timestamp','new_window', 'num_window')
drop = c('X', 'user_name', 'raw_timestamp_part_1', 'raw_timestamp_part_2',
'cvtd_timestamp','new_window', 'num_window')
drop
data = data[,!(names(data) %in% drops)]
drops = c('X', 'user_name', 'raw_timestamp_part_1', 'raw_timestamp_part_2',
'cvtd_timestamp','new_window', 'num_window')
data = data[,!(names(data) %in% drops)]
set.seed(666)
# Read Data
pmlData <- read.csv("pml-training.csv")
pmlFinalTest <- read.csv("pml-testing.csv")
# Test which variables are common to both dataset
common = intersect(names(pmlData),names(pmlFinalTest))
# Select data only when "new_windows" = yes
data = pmlData[pmlData$new_window == 'yes', ]
# Drop unnecessary columns
drops = c('X', 'user_name', 'raw_timestamp_part_1', 'raw_timestamp_part_2',
'cvtd_timestamp','new_window', 'num_window')
data = data[,!(names(data) %in% drops)]
View(data)
data = pmlData[pmlData$new_window == 'yes', ]
drops = c('X', 'user_name', 'raw_timestamp_part_1', 'raw_timestamp_part_2',
'cvtd_timestamp','new_window', 'num_window', 'kurtosis_yaw_belt',
'skewness_yaw_belt', 'kurtosis_yaw_dumbbell', 'skewness_yaw_dumbbell',
'amplitude_yaw_dumbbell', 'amplitude_yaw_belt')
data = data[,!(names(data) %in% drops)]
View(data)
str(data)
asNumeric <- function(x) as.numeric(as.character(x))
factorsNumeric <- function(d) modifyList(d, lapply(d[, sapply(d, is.factor)],
asNumeric))
data <- factorsNumeric(data)
warnings()
View(data)
str(data)
system.time({
modelFit <- train(classe ~ .,
data = trainning,
#trControl=train_control, ntree=100,
preProcess=c("knnImpute")
verbose=FALSE,
method=c("glm"))
})
system.time({
modelFit <- train(classe ~ .,
data = trainning,
#trControl=train_control, ntree=100,
preProcess=c("knnImpute"),
verbose=FALSE,
method=c("glm"))
})
# Create trainning datasets
inTrain <- createDataPartition(y = data$classe, p = 0.8, list = F)
trainning <- pmlData[inTrain,]
testing <- pmlData[-inTrain,]
set.seed(666)
# Read Data
pmlData <- read.csv("pml-training.csv")
pmlFinalTest <- read.csv("pml-testing.csv")
# Test which variables are common to both dataset
common = intersect(names(pmlData),names(pmlFinalTest))
# Select data only when "new_windows" = yes
data <- pmlData[pmlData$new_window == 'yes', ]
# Drop unnecessary columns
drops <- c('X', 'user_name', 'raw_timestamp_part_1', 'raw_timestamp_part_2',
'cvtd_timestamp','new_window', 'num_window', 'kurtosis_yaw_belt',
'skewness_yaw_belt', 'kurtosis_yaw_dumbbell', 'skewness_yaw_dumbbell',
'amplitude_yaw_dumbbell', 'amplitude_yaw_belt')
data <- data[,!(names(data) %in% drops)]
asNumeric <- function(x) as.numeric(as.character(x))
factorsNumeric <- function(d) modifyList(d, lapply(d[, sapply(d, is.factor)],
asNumeric))
data <- factorsNumeric(data)
inTrain <- createDataPartition(y = data$classe, p = 0.8, list = F)
trainning <- pmlData[inTrain,]
testing <- pmlData[-inTrain,]
inTrain <- createDataPartition(y = data$classe, p = 0.8, list = F)
require(DMwR)
install.packages("DMwR")
require(DMwR)
data<-knnImputation(data,k=5)
data<-knnImputation(data,k=10)
data <- data[,!(names(data) %in% drops)]
data<-knnImputation(data,k=10)
data <- data[,!(names(data) %in% drops)]
asNumeric <- function(x) as.numeric(as.character(x))
factorsNumeric <- function(d) modifyList(d, lapply(d[, sapply(d, is.factor)],
asNumeric))
data <- factorsNumeric(data)
require(DMwR)
data<-knnImputation(data,k=3)
data<-knnImputation(data,k=2)
complete.cases(data)
data <- pmlData[pmlData$new_window == 'yes', ]
drops <- c('X', 'user_name', 'raw_timestamp_part_1', 'raw_timestamp_part_2',
'cvtd_timestamp','new_window', 'num_window', 'kurtosis_yaw_belt',
'skewness_yaw_belt', 'kurtosis_yaw_dumbbell', 'skewness_yaw_dumbbell',
'amplitude_yaw_dumbbell', 'amplitude_yaw_belt')
data <- data[,!(names(data) %in% drops)]
# Convert variables to factor
asNumeric <- function(x) as.numeric(as.character(x))
factorsNumeric <- function(d) modifyList(d, lapply(d[, sapply(d, is.factor)],
asNumeric))
data <- factorsNumeric(data)
complete.cases(data)
View(data)
data[,colSums(is.na(data))]
data[,colSums(is.na(data)) < nrow(df) * 0.5]
data[,colSums(is.na(data)) < 0]
colSums(is.na(data))
data[,colSums(is.na(data)) == 0]
data <- data[,colSums(is.na(data)) == 0]
View(data)
set.seed(666)
# Read Data
pmlData <- read.csv("pml-training.csv")
pmlFinalTest <- read.csv("pml-testing.csv")
# Select data only when "new_windows" = yes
data <- pmlData[pmlData$new_window == 'yes', ]
# Drop unnecessary columns
drops <- c('X', 'user_name', 'raw_timestamp_part_1', 'raw_timestamp_part_2',
'cvtd_timestamp','new_window', 'num_window', 'kurtosis_yaw_belt',
'skewness_yaw_belt', 'kurtosis_yaw_dumbbell', 'skewness_yaw_dumbbell',
'amplitude_yaw_dumbbell', 'amplitude_yaw_belt')
data <- data[,!(names(data) %in% drops)]
# Convert variables to factor
asNumeric <- function(x) as.numeric(as.character(x))
factorsNumeric <- function(d) modifyList(d, lapply(d[, sapply(d, is.factor)],
asNumeric))
data <- factorsNumeric(data)
# Remove columns that have NAs
data <- data[,colSums(is.na(data)) == 0]
# Create trainning datasets
inTrain <- createDataPartition(y = data$classe, p = 0.8, list = F)
trainning <- pmlData[inTrain,]
testing <- pmlData[-inTrain,]
inTrain <- createDataPartition(y = data$classe, p = 0.8, list = F)
# Select data only when "new_windows" = yes
data <- pmlData[pmlData$new_window == 'yes', ]
data$classe
# Select data only when "new_windows" = yes
data <- pmlData[pmlData$new_window == 'yes', ]
classe <- data$classe
# Drop unnecessary columns
drops <- c('X', 'user_name', 'raw_timestamp_part_1', 'raw_timestamp_part_2',
'cvtd_timestamp','new_window', 'num_window', 'kurtosis_yaw_belt',
'skewness_yaw_belt', 'kurtosis_yaw_dumbbell', 'skewness_yaw_dumbbell',
'amplitude_yaw_dumbbell', 'amplitude_yaw_belt')
data <- data[,!(names(data) %in% drops)]
# Convert variables to factor
asNumeric <- function(x) as.numeric(as.character(x))
factorsNumeric <- function(d) modifyList(d, lapply(d[, sapply(d, is.factor)],
asNumeric))
data <- factorsNumeric(data)
# Select data only when "new_windows" = yes
data <- pmlData[pmlData$new_window == 'yes', ]
classe <- data$classe
# Drop unnecessary columns
drops <- c('X', 'user_name', 'raw_timestamp_part_1', 'raw_timestamp_part_2',
'cvtd_timestamp','new_window', 'num_window', 'kurtosis_yaw_belt',
'skewness_yaw_belt', 'kurtosis_yaw_dumbbell', 'skewness_yaw_dumbbell',
'amplitude_yaw_dumbbell', 'amplitude_yaw_belt')
data <- data[,!(names(data) %in% drops)]
# Convert variables to factor
asNumeric <- function(x) as.numeric(as.character(x))
factorsNumeric <- function(d) modifyList(d, lapply(d[, sapply(d, is.factor)],
asNumeric))
data <- factorsNumeric(data)
# Remove columns that have NAs
data <- data[,colSums(is.na(data)) == 0]
data <- data.frame(classe, data)
View(data)
inTrain <- createDataPartition(y = data$classe, p = 0.8, list = F)
trainning <- pmlData[inTrain,]
testing <- pmlData[-inTrain,]
system.time({
modelFit <- train(classe ~ .,
data = trainning,
#trControl=train_control, ntree=100,
preProcess=c("knnImpute"),
verbose=FALSE,
method=c("glm"))
})
# Fit model: random forest
system.time({
modelFit <- train(classe ~ .,
data = trainning,
#trControl=train_control, ntree=100,
verbose=FALSE,
method=c("glm"))
})
warnings()
str(data)
str(trainning)
# Remove columns that have NAs
data <- data[,colSums(is.na(data)) == 0]
data <- pmlData[pmlData$new_window == 'yes', ]
classe <- data$classe
# Drop unnecessary columns
drops <- c('X', 'user_name', 'raw_timestamp_part_1', 'raw_timestamp_part_2',
'cvtd_timestamp','new_window', 'num_window', 'kurtosis_yaw_belt',
'skewness_yaw_belt', 'kurtosis_yaw_dumbbell', 'skewness_yaw_dumbbell',
'amplitude_yaw_dumbbell', 'amplitude_yaw_belt')
data <- data[,!(names(data) %in% drops)]
# Convert variables to factor
asNumeric <- function(x) as.numeric(as.character(x))
factorsNumeric <- function(d) modifyList(d, lapply(d[, sapply(d, is.factor)],
asNumeric))
data <- factorsNumeric(data)
# Remove columns that have NAs
data <- data[,colSums(is.na(data)) == 0]
data <- data.frame(classe, data)
str(data)
# Create trainning datasets
inTrain <- createDataPartition(y = data$classe, p = 0.8, list = F)
trainning <- pmlData[inTrain,]
testing <- pmlData[-inTrain,]
inTrain <- createDataPartition(y = data$classe, p = 0.8, list = F)
training <- pmlData[inTrain,]
testing <- pmlData[-inTrain,]
str(training)
set.seed(666)
# Read Data
pmlData <- read.csv("pml-training.csv")
pmlFinalTest <- read.csv("pml-testing.csv")
# Test which variables are common to both dataset
common = intersect(names(pmlData),names(pmlFinalTest))
# Select data only when "new_windows" = yes
data <- pmlData[pmlData$new_window == 'yes', ]
classe <- data$classe
# Drop unnecessary columns
drops <- c('X', 'user_name', 'raw_timestamp_part_1', 'raw_timestamp_part_2',
'cvtd_timestamp','new_window', 'num_window', 'kurtosis_yaw_belt',
'skewness_yaw_belt', 'kurtosis_yaw_dumbbell', 'skewness_yaw_dumbbell',
'amplitude_yaw_dumbbell', 'amplitude_yaw_belt')
data <- data[,!(names(data) %in% drops)]
# Convert variables to factor
asNumeric <- function(x) as.numeric(as.character(x))
factorsNumeric <- function(d) modifyList(d, lapply(d[, sapply(d, is.factor)],
asNumeric))
data <- factorsNumeric(data)
# Remove columns that have NAs
data <- data[,colSums(is.na(data)) == 0]
# Add classe variable back
data <- data.frame(classe, data)
inTrain <- createDataPartition(y = data$classe, p = 0.8, list = F)
training <- data[inTrain,]
testing <- data[-inTrain,]
system.time({
modelFit <- train(classe ~ .,
data = training,
#trControl=train_control, ntree=100,
verbose=FALSE,
method=c("glm"))
})
system.time({
modelFit <- train(classe ~ .,
data = training,
#trControl=train_control, ntree=100,
verbose=FALSE,
method=c("rpart"))
})
warnings()
str(data)
str(training)
system.time({
modelFit <- train(classe ~ .,
data = training,
#trControl=train_control, ntree=100,
verbose=FALSE,
method=c("rpart"))
})
warnings()
system.time({
modelFit <- train(classe ~ .,
data = training,
#trControl=train_control, ntree=100,
verbose=FALSE,
method=c("glm"))
})
warnings()
system.time({
modelFit <- train(classe ~ .,
data = training,
#trControl=train_control, ntree=100,
verbose=FALSE,
method=c("rf"))
})
# Test the model
predictions <- predict(modelFit, newdata = testing)
confusionMatrix(predictions, testing$classe)
install.packages(c("caret", "knitr", "manipulate", "plyr", "Rcpp", "rmarkdown", "stringr"))
install.packages(c("caret", "knitr", "manipulate", "plyr", "Rcpp",
))
install.packages(c("caret", "knitr", "manipulate", "plyr", "Rcpp"))
install.packages(c("caret", "knitr", "manipulate", "plyr", "Rcpp"))
install.packages(c("caret", "knitr", "manipulate", "plyr", "Rcpp"))
install.packages(c("caret", "knitr", "manipulate", "plyr", "Rcpp"))
install.packages(c("caret", "knitr", "manipulate", "plyr", "Rcpp"))
library("caret")
set.seed(666)
# Read Data
pmlData <- read.csv("pml-training.csv")
pmlFinalTest <- read.csv("pml-testing.csv")
# Test which variables are common to both dataset
common = intersect(names(pmlData),names(pmlFinalTest))
# Select data only when "new_windows" = yes
data <- pmlData[pmlData$new_window == 'yes', ]
classe <- data$classe
# Drop unnecessary columns
drops <- c('X', 'user_name', 'raw_timestamp_part_1', 'raw_timestamp_part_2',
'cvtd_timestamp','new_window', 'num_window', 'kurtosis_yaw_belt',
'skewness_yaw_belt', 'kurtosis_yaw_dumbbell', 'skewness_yaw_dumbbell',
'amplitude_yaw_dumbbell', 'amplitude_yaw_belt')
data <- data[,!(names(data) %in% drops)]
# Convert variables to factor
asNumeric <- function(x) as.numeric(as.character(x))
factorsNumeric <- function(d) modifyList(d, lapply(d[, sapply(d, is.factor)],
asNumeric))
data <- factorsNumeric(data)
# Remove columns that have NAs
data <- data[,colSums(is.na(data)) == 0]
# Add classe variable back
data <- data.frame(classe, data)
# Imputting data
#require(DMwR)
#data<-knnImputation(data,k=2)
# Preprocess training data
# Create trainning datasets
inTrain <- createDataPartition(y = data$classe, p = 0.8, list = F)
training <- data[inTrain,]
testing <- data[-inTrain,]
system.time({
modelFit <- train(classe ~ .,
data = training,
#trControl=train_control, ntree=100,
verbose=FALSE,
method=c("rf"))
})
predictions <- predict(modelFit, newdata = testing)
confusionMatrix(predictions, testing$classe)
